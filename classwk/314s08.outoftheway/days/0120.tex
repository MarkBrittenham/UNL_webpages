
%\input amstex


\magnification=1200

\voffset=-.6in
\hoffset=-.5in
\hsize = 7.5 true in
\vsize=10.6 true in

%\voffset=1.4in
%\hoffset=-.5in
%\hsize = 10.2 true in
%\vsize=8 true in

%\loadmsbm
\input colordvi

\nopagenumbers
\parindent=0pt

\overfullrule=0pt


\def\ctln{\centerline}
\def\u{\underbar}
\def\ssk{\smallskip}
\def\msk{\medskip}
\def\bsk{\bigskip}
\def\hsk{\hskip.1in}
\def\hhsk{\hskip.2in}
\def\dsl{\displaystyle}
\def\hskp{\hskip1.5in}

\def\lra{$\Leftrightarrow$ }





\ctln{\bf Math 314 Matrix Theory}

\ssk

\ctln{January 20, 2005}

\msk

In {\it reduced row echelon form (RREF)}, each of the resulting equations can be interpreted as reading
either

\msk

(1) \hskip.3in (bound variable) = (equation involving free variables)

\ssk

(2) \hskip.3in $0$ $=$ $0$ \hskip.3in [which gives no new information]

\ssk

(3)  \hskip.3in $0$ $=$ $1$ \hskip.3in [so the system is inconsistent]

\msk

If the system is consistent, then {\it any} assignment of values to the free variables, leads, via the
equations above, to a specific value of each of the bound variables, which gives a solution to
the original system of equations.

\bsk

Several things to note:

\ssk

It is a fact, which we will establish later, that each augmented matrix has exactly one RREF;
it doesn't matter what order we do our row reductions in, they will lead to the exact same
place. 

\msk

Our analysis above already allows us to understand solutions to systems in a general way; an inconsistent
system has no solutions, a consistent system with no free variables has exactly one solution (the equations
then read (bound variable) = (constant)), and a consistent system with one or more free variables has 
infinitely many distinct solutions (corresponding to different values of the free variables).

\bsk

{\it Linear systems as vector equations}

\bsk

A {\it vector} $\left[\matrix{a\cr b\cr}\right]$ in ${\bf R}^2$ is an arrow with tail at $(x_0,y_0)$ and head at 
$(x_0+a,y_0+b)$ . We generally think of vectors with the same description $a,b$ , as the same, even if their 
tails are in different places. When we want uniformity, we put the tail at $(0,0,)$, so the vector corresponds
to the point $(a,b)$ . More generally, we can think of vectors in ${\bf R}^n$ as having tails at $(0,\ldots,0)$ and
heads at $(a_1,\ldots a_n)$ . {\it Vector addition} and {\it scalar multiplication} can be defined as they are
in ${\bf R}^2$, working component by component: 

\msk

$\left[\matrix{a_1\cr \vdots\cr a_n\cr}\right] + \left[\matrix{b_1\cr \vdots\cr b_n\cr}\right] = 
\left[\matrix{a_1+b_1\cr \vdots\cr a_n+b_n\cr}\right]$
\hsk and \hsk
$c\left[\matrix{a_1\cr \vdots\cr a_n\cr}\right] = \left[\matrix{c a_1\cr \vdots\cr c a_n\cr}\right]$.

\bsk

A system of linear equations 
$\matrix{a_{11}x_1+\cdots +a{1n}x_n = b_1\cr \vdots\cr a_{m1}x_1+\cdots +a{mn}x_n = b_m}$ 
can be re-written as 
$x_1\left[\matrix{a_{11}\cr \vdots \cr a_{m1}\cr}\right] + \cdots x_n\left[\matrix{a_{1n}\cr \vdots \cr a_{mn}\cr}\right] 
= \left[\matrix{b_1 \cr \vdots \cr b_m\cr}\right]$

\msk

The left side is a {\bf linear combination} of vectors in ${\bf R}^m$ . Essentially, a solving a 
system of equations amounts to finding the appropriate linear combination of {\it column
vectors} from the coefficient latrix of the system, which equals the {\it target vector}. So a
system of linear equations can be interpreted as a {\it single} vector equation in ${\bf R}^m$.
This will be an important interpretation for us as we move forward!


\vfill\end

