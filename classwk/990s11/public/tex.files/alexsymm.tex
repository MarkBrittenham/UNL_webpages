\magnification=1200
\voffset=.5in

\input amstex
\loadmsbm

\define\dl{\displaystyle}
\define\ctln{\centerline}
\define\ssk{\smallskip}
\define\msk{\medskip}
\define\bsk{\bigskip}
\define\bbr{{\Bbb R}}
\define\bbz{{\Bbb Z}}
\define\ii{{\italic i}}
\define\ubr{\underbar}
\define\lra{$\Leftrightarrow$}
\define\ra{\rightarrow}
\define\vskt{\vskip-10pt}
\define\del{\partial}

\overfullrule=0pt
\parindent=0pt
\parskip=10pt


\input epsf.tex

\ctln{\bf The Alexander polynomial is symmetric: $\Delta_K(t)=\Delta_k(t^{-1})$}

\ssk

\ctln{following R.H. Fox and G. Torres [Ann. Math. {\bf 59} \#2 (3-1954) 211-218]}

\bsk

In previous lectures we have shown that if we define the (single-variable) Alexander
for \ubr{oriented} knots and links, then the effect of reversing the
orientations on all components is, up to multiplication by $\pm t^n$, to replace
the variable $t$ with its inverse $t^{-1}$. We now show that the Alexander polynomial 
itself is invariant (up to the same multiplications by units) under this
transformation. We shall do this using the Fox derivative approach to $\Delta_K(t)$.

\msk

Fox's approach begins with a presentation $G=<A|R>$ for the fundamental group 
$\pi_1(S^3\setminus K)$ of the complement of a knot or link $K$, together
with a choice of surjective homomorphism $G\ra\bbz$. For consistency, we will
make a canonical choice using the orientation of $K$, by insisting that a loop
which circles the knot using the righthand rule (curling around the knot
in the direction of our fingers when the thumb is aligned with the orientation)
is mapped to the generator $t$, and a loop travelling the opposite direction 
is mapped to $t^{-1}$. [These are basically the Wirtinger generators (and their
inverses), which we noted previously are carried under abelianization to the
generator or its inverse.]

\ssk

To compute $\Delta_K(t)$ we start by computing the {\it Fox derivatives} of
the relators $r_j=x_{j_1}^{\epsilon_1}\cdots x_{j_k}^{\epsilon_k}$ with 
respect to each of the generators $x_i$, by accumulating terms in an 
(initially empty) sum as follows: reading $r_j$ from left
to right, each occurance of the generator $x_i$ occurs as
$r_j=ux_iv$ or $r_j=ux_i^{-1}v$. In the first case we add $u$, and in the 
second case we add $-ux_i^{-1}$. The resulting sum should be thought of
as an element in the group ring of the free group $F(X)$ generated by
the symbols $X$, and is denoted $\dl {{\del r_j}\over{\del x_i}}$. 
The canonical homomorphism $\psi$ from $F(X)$ to $G$ (sending each $x_i$ to $x_i$),
followed by our chosen homomomorphism $\phi$ from $G$ to $\bbz=<t|>$, induces ring
homomorphisms between their group rings; their composition sends each
$\dl {{\del r_j}\over{\del x_i}}$ to a (Laurent) polynomial in $t$ with
coefficients in $\bbz$. 

\ssk

We have seen that these polynomials, when assembled into a `Jacobian' matrix

\ssk

\ctln{$\dl J=\big({{\del r_j}\over{\del x_i}}\big)^{\psi\phi}$,}

\ssk

yields a matrix which depends 
upon the presentation for $G$ chosen. But any two (finite) presentations of $G$ can
be transformed one to the other by Teitze transformations, and the effect on 
the Jacobian matrix under these moves can be codified. In particular, the 
resulting moves on the Jacobian do not change (for J an $n\times m$ matrix)
the {\it ideal (in $\bbz[\bbz]$) generated by the 
$(m-k)\times(m-k)$ minor determinants} of $J$. Then taking the
ideal $I_1$ generated by the $(m-1)\times(m-1)$ minors, $I_1$ is principal,
and a generator for $I_1$ is the Alexander polynomial $\Delta_K(t)$ (well defined up
to multiplication by a unit $\pm t^n$ in $\bbz[\bbz]$).

\msk

To prove that the Alexander polynomial is symmmetric, Fox and Torres construct
a pair of `dual' presentations for $G$, as follows: Give a diagram of 
the knot/link $K$, partition the loop representing $K$ into alternating
`oversegments' and `undersegments': an oversegment passes over every crossing
in the diagram that it meets, and an undersegment passes under. For example,
in the Wirtinger presentation we take the oversegments to be the entire
segment running between successive undercrossings as we traverse the knot
using the orientation, and the undersegments are the remaining short segments
passing under each crossing. The figure below shows a more effecient way to 
do this for the knot $8_{20}$. 


\leavevmode

\epsfxsize=4in
\ctln{\epsfbox{8.20oub.eps}}

\msk

The basic idea is that an over/under representation of $K$ enables us to 
build \ubr{two} `dual' presentations of $G$, which we will use to establish 
our symmetry result. The idea is that the oversegments give generators of
a presentation for $G$
and the undersegments provide the relators for the presentation; the opposite
is true for the second presentation. The idea is really the same as
for the Wirtinger presentation: choosing our nose as the basepoint for
$\pi(S^3\setminus K)$, each oversegment corrresponds to a loop which runs from 
our nose to a point just to the right of the segment (using the orientation
on K to determine `right'), passes under the oversegment, and then directly
back to our nose. The relators are determined by drawing a loop around each 
undersegment and which closely follows it. From a fixed starting point we will cross
a sequence of oversegments; reading off this sequence (thinking of ourselves
as actually travelling slightly below the plane of the knot projection)
reads off a word in the generators which is our relator. To be precise about 
things, we will orient the loops around the undersegments to run counterclockwise
around and oversegment, and clockwise around an undersegment. The proof that
$G$ is presented with generators given by the loops around oversegments and
relators given by reading off the words surrounding undersegments is essentially
identical to that for the Wirtinger presentation; in that case the loop under
and surrounding a crossing is identical to the loop surrounding the undersegment.
[Of course, we did not present that proof, but any discussion of the Wirtinger
presentation can be adapted with no change to this more general setting.

\ssk

Oversegments and loops around undersegments therefore gives us one presentation.
(Note that we, mostly for convenience, label the segments around the projection
as $x_1,y_1,x_2,y_2,\ldots$.)
The `dual' presentation comes essentially from changing our point of view to
\ubr{behind} the projection plane. Then what we have called undersegments
\ubr{become} oversegments and vice versa. So we can apply the same argument
to build a second presentation. We will however retain a viewpoint from the
front of the projection plane, in order to discuss both presentation at the 
same time. The `under' presentation will have generators $y_j$ corresponding to 
undersegments; the specific loop representing the generator 
we will think of as running from right to
left across the undersegment, but this loop is now starting at a point
lying \ubr{behind} the projection plane, and passing \ubr{over} the strand.
{\bf The important point:} as a result, when we map to $\bbz$ using the orientation,
each $y_j$ will map to $t^{-1}$ (and not $t$). The relators for the underpresentation
come from reading the words in the $y_j$ that the loops running around the oversegments
(and thought of as lying slightly \ubr{above} the projection plane) spell out.
It is essentially because our basepoint for the underpresentation lies behind the
projection plane that we orient the loops around the oversegments in the opposite
direction to the loops around undersegments for the overpresentation: from the
perspective of the basepoint, we are really reading around the loops in the same 
directions.

\msk

The basic point here is that the generators for the overpresentation come from the 
oversegments and the relators come from the undersegments. For the underpresentation
it is the exact opposite: the generators come from the undersegments and the
relators come from the oversegments. Our plan then is to compare computations of
$\dl {{\del r_j}\over{\del x_i}}$ and
$\dl {{\del s_i}\over{\del y_j}}$, and show that they are \ubr{identical} (after
replacing $t$ with $t^{-1}$). This will show that the Jacobian matrices for the
two presentations (after changing the base rings to $\bbz[\bbz]$) satisfy
$(J_{\text{over}})^{\psi\phi}(t)=[(J_{\text{under}})^{\psi\phi}(t^{-1})]^T$. Therefore,
since a matrix and its transpose have the same determinant, 
(after changing the variable) the ideals generated by the codimenskion-1
minor determinants are \ubr{identical}, and therefore have the same
generator, i.e., $\Delta_K(t)=\Delta_K(t^{-1})$, as desired.

\msk

Each of the two Fox derivatives $\dl {{\del r_j}\over{\del x_i}}$ and
$\dl {{\del s_i}\over{\del y_j}}$ is a formal sum, over the 
specific instances of the variable $x_i$
(resp. $y_j$) in the relator $r_j$ (resp $x_i$). These instances occur in one of 
\ubr{three} ways, which we will treat in turn. The idea is that each instance can be 
paired with a corresponding instance in the \ubr{other} Fox derivative, and
that the contributions to the Fox derivatives of each is \ubr{identical} once
we replace $t$ with $t^{-1}$ in one of the two computations. 

\ssk

First, note that if the oversegment $x_i$ and the undersegment $y_j$ do not meet,
then $x_i$ does not appear in $r_j$ and $y_j$ does not appear in $s_i$,
so both Fox derivative are $0$, and so are equal (after replacing $t$ in one 
with $t^{-1}$...!). So in what follows we need only concern ourselves
with cases where the two segments (and so, in particular, the two 
loops $r_j$ and $s_i$) intersect.

\msk

The first instance, which we did not explore in class, is the fact that the loops
representing $r_j$ and $s_i$ must actually represent \ubr{based} loops, using
a fixed and common basepoint (one for all of the $r_j$, another for all of the $s_i$).
This is in fact the {\bf single greatest pitfall} in carrying out fundamental
group computations; we must use a single basepoint throughout the computation, which in
essence really means that for a loop such as the $r_j$ and $s_i$ we wish to use, we should
read the word starting at a fixed point along the loop, 
and must pre- and post-pend a path to a fixed
and common basepoint. (Failure to do so means that a word we might be using is
actually a \ubr{conjugate} of the word we really \ubr{should} be using.) But this is 
not nearly so difficult an issue with the Fox calculus as it might seem. Because
of the fact that the Fox derivative satisfies (using the shorthand notation
$u_x$ for the Fox derivative of $u$ w.r.t. $x$) $(uv)_x=u_x+u(v_x)$, we find that
for a relator $r$,

\ssk

\ctln{$(uru^{-1})_x=u_x+u(r_x)+ur(u^{-1})_x=u_x+ur_x-uru^{-1}u_x$}

\ssk

But when we push forward from $F(X)$ to $\bbz$, every relator $r$ is sent to ($1$ in $G$,
and so is sent to) $1$ in $\bbz$, so $uru^{-1}$ is sent to $uu^{-1}=1$, so 
$(uru^{-1})_x=u(r_x)$. The effect of conjugation on the Fox derivative of a relator,
that is, the effect of the choice of path to the basepoint, 
is therefore to multiply by the (image in $\bbz$ of the) conjugator (on the left). 

\ssk

For our comparison of Fox derivatives, we choose fixed basepoints lying above/below 
the same point $p$ in the projection plane and away from the knot projection, and choose
fixed paths $\gamma_j,\delta_i$ from $p$ to each of the loops $r_j$ and $s_i$ lying slightly
below/above the projection plane; the paths
$\gamma_j$ should avoid the undersegments (and therefore pass under the oversegments),
and the $\delta_i$ should similarly avoid the oversegments. [This is possible because,
in the projection plane, the complement of the set of oversegments is connected (and the
same for the collection of undersegments); this is most easily seen by induction on the 
number of segments.]

\vskip-10pt

\leavevmode

\epsfxsize=4in
\ctln{\epsfbox{baseptb.eps}}

\msk

But! We \ubr{really} would like to use a basepoint $q$ lying at the intersection of
$r_j$ and $s_i$ (for reasons that will become apparent). That is, in our computations
and comparisons,
we would like to simply read around the relator starting from a common point
of intersection. So we \ubr{move} the endpoints of $\gamma_j$ and $\delta_i$, by
appending a segment of the loops $r_j$ and $s_i$, to have these loops start and end at $q$.
(Essentially, we are writing $\gamma_jr_j\gamma_j^{-1}$ as 
$\gamma_juv\gamma_j^{-1}=[\gamma_ju]vu[u^{-1}\gamma_j^{-1}]=(\gamma_ju)vu(\gamma_ju)^{-1}$;
these two words are equal in the free group and so have the same Fox derivative.)
But now these appended paths, which we will still call $\gamma_j$ and $\delta_i$,
begin and end at the same point in the projection plane, and so form a
loop $\eta$. The $\gamma_j$ half of this loop misses the undersegments, and the $\delta_i$
half misses the oversegments. Therefore, since the points of intersection
of the knot itself with the loop $\eta$ must pair up (by following the segments around the
knot projection), the net number of times that each of $\gamma_j$ and $\delta_i$
cross their respective segments {\it from right to left} must be the same; either
the knot recrosses the two paths at points with the same `color', in which 
case it must pass across 
the \ubr{same} path in \ubr{opposite}
directions (yielding a net contribution of $t^0=1$ in $\bbz[\bbz]$), 
or the knot recrosses with the opposite color, in which case
it crosses the \ubr{other} path in the \ubr{same} direction, pairing up the two crossings.
[These facts are apparent from the figure: to establish them formally we can argue that
the loop $\gamma*\overline{\delta}$ is null-homotopic in the 
plane, as is the projection of (every component of) $K$, and so 
(using homology) they intersect algebraically
$0$ times. So, algebraically, $\gamma$ and $\delta$ meet $K$ the
same number of times. But $\gamma$ only meets oversegments, and $\delta$ meets only 
undersegments, and so they each meet each the same net number of times.]

\ssk

The net effect is that the exponent sum of the $x$'s in $\gamma_j$ must equal
the exponent sum of the $y$'s in $\delta_i$. If this common exponent sum is $n$, then 
conjugating $r_j$ by $\gamma_j$ multiplies the Fox derivative by $t^n$, and 
conjugating $s_i$ by $\delta_i$ multiplies the Fox derivative by $t^{-n}$ (since
each of the $y_j$ is sent to $t^{-1}$). Thus to take into account the path
to a more conveniently chosen basepoint, each Fox derivative 
changes in a way consistent with our goal;
if the terms of the derivative are equal (after variable change), then the conjugated
terms are, as well. So in what follows we can ignore the effect of the path
from the basepoint, and act as if the basepoint lies where we want it, 
at the intersection of 
the two loops $r_j$ and $s_i$.

\leavevmode

\epsfxsize=2.8in
\ctln{\epsfbox{crossb.eps}}

\msk

The most typical instances of the $x_i,y_j$ in our relators occur where the 
oversegment $x_i$ \ubr{crosses} the undersegment
$y_j$. Each such crossing generates a pair of instances of 
$x_i$ in $r_j$ and a pair of instances of $y_j$ in $s_i$. Choosing our basepoint
$q$ to lie at the intersection of the two loops as indicated in the figure above, we
can read off the relators as

\ssk

\ctln{$r_j=x_i^{-1}ux_ju^{-1}x_ivx_{j+1}^{-1}v^{-1}$ and 
$s_i=y_jay_{i-1}^{-1}a^{-1}y_j^{-1}by_ib^{-1}$}

\ssk

If we then compute the contributions to the Fox derivatives of each of the two relators,
we find

\ssk

For $r_j$ : $-x_i^{-1}+x_i^{-1}ux_ju^{-1}$
\hskip.2in and
for $s_i$ : $1-y_jay_{i-1}^{-1}a^{-1}y_j^{-1}$

\ssk

Then sending the $x$'s to $t$ and the $y$'s to $t^{-1}$, $u$ is sent to $t^m$ and $a$ is sent to 
$t^m$, so we get the contributions:

\ssk

For $r_j$ : $-t^{-1}+t^{-1}t^ntt^{-n}=1-t^{-1}$
\hskip.2in and
for $s_i$ : $1-t^{-1}t^mtt^{-m}t=1-t$

\ssk

So the contribution, as a function of $t$, to 
$\dl {{\del r_j}\over{\del x_i}}$ is equal to the contribution to
$\dl {{\del s_i}\over{\del y_j}}$, evaluated at $t^{-1}$.


\leavevmode

\epsfxsize=4.8in
\ctln{\epsfbox{ends.eps}}

The final contribution comes from the fact that $r_j$ contains instances of $x_j$ and $x_{j+1}$,
and $s_i$ contains instances of $y_{i-1}$ and $y_i$. So if $i=j$, or if $i=j+1$ (so $j=i-1$),
each has a single additional instance of the variable we are computing the Fox derivative
with respect to. These occur when the segments we are using follow one after
the other, as in the figure above. In these cases, choosing our basepoint for comparison
to be the point $q$ depicted in the figure, the contribution of these
instances of each variable can be computed as

\ssk

$i=j+1$: $\dl {{\del r_j}\over{\del x_i}}$ has an additional $-x_i^{-1}$, evaluating to $-t^{-1}$,
while $\dl {{\del s_i}\over{\del y_j}}$ has an additional $-y_{i-1}^{-1}$, evaluating to $-t$;

\ssk

$i=j$: $\dl {{\del r_j}\over{\del x_i}}$ has an additional $x_i$, evaluating to $t$,
while $\dl {{\del s_i}\over{\del y_j}}$ has an additional $y_{i}$, evaluating to $t^{-1}$.

\ssk

As before, the first is then equal to the second, evaluated at $t^{-1}$.

\ssk

Therefore, we find that for these dual presentations, the Jacobian matrices satisfy

\ssk

\ctln{$(J_{\text{over}})^{\psi\phi}(t)=[(J_{\text{under}})^{\psi\phi}(t^{-1})]^T$.}

\ssk

As remarked above, this implies that the Alexander ideals have generators that 
are equal to one another, after the variable $t$ in one is replaced with $t^{-1}$.
Since they are also equal to one another (being the result of the Fox calculus
applied to two presentations of the same group), we conclude that

\msk

\ctln{$\Delta_K(t)=\Delta_k(t^{-1})$,}

\ssk

up to multiplication by units $\pm t^n$ in the group ring $\bbz[\bbz]$.






\vfill
\end











